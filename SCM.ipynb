{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan0931/E-commerce-delivery/blob/main/SCM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ3xRz4bur1m",
        "outputId": "33e513f3-0407-4b3d-e262-010c0aaa9be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.8-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting web3\n",
            "  Downloading web3-7.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting eth-account\n",
            "  Downloading eth_account-0.13.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting py-solc-x\n",
            "  Downloading py_solc_x-2.0.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting eth-tester\n",
            "  Downloading eth_tester-0.13.0b1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (7.34.0)\n",
            "Collecting py-evm\n",
            "  Downloading py_evm-0.12.1b1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Collecting eth-abi>=5.0.1 (from web3)\n",
            "  Downloading eth_abi-5.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting eth-hash>=0.5.1 (from eth-hash[pycryptodome]>=0.5.1->web3)\n",
            "  Downloading eth_hash-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting eth-typing>=5.0.0 (from web3)\n",
            "  Downloading eth_typing-5.2.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting eth-utils>=5.0.0 (from web3)\n",
            "  Downloading eth_utils-5.3.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting hexbytes>=1.2.0 (from web3)\n",
            "  Downloading hexbytes-1.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.12/dist-packages (from web3) (3.13.2)\n",
            "Requirement already satisfied: pydantic>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from web3) (2.11.10)\n",
            "Collecting types-requests>=2.0.0 (from web3)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: websockets<16.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from web3) (15.0.1)\n",
            "Collecting pyunormalize>=15.0.0 (from web3)\n",
            "  Downloading pyunormalize-17.0.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting bitarray>=2.4.0 (from eth-account)\n",
            "  Downloading bitarray-3.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
            "Collecting eth-keyfile<0.9.0,>=0.7.0 (from eth-account)\n",
            "  Downloading eth_keyfile-0.8.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting eth-keys>=0.4.0 (from eth-account)\n",
            "  Downloading eth_keys-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting eth-rlp>=2.1.0 (from eth-account)\n",
            "  Downloading eth_rlp-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting rlp>=1.0.0 (from eth-account)\n",
            "  Downloading rlp-4.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting ckzg>=2.0.0 (from eth-account)\n",
            "  Downloading ckzg-2.1.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (887 bytes)\n",
            "Requirement already satisfied: semantic_version>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from eth-tester) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n",
            "Collecting cached-property>=1.5.1 (from py-evm)\n",
            "  Downloading cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting eth-bloom>=1.0.3 (from py-evm)\n",
            "  Downloading eth_bloom-3.1.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting lru-dict>=1.1.6 (from py-evm)\n",
            "  Downloading lru_dict-1.4.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting py-ecc>=8.0.0 (from py-evm)\n",
            "  Downloading py_ecc-8.0.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting trie>=2.0.0 (from py-evm)\n",
            "  Downloading trie-3.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.22.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Collecting parsimonious<0.11.0,>=0.10.0 (from eth-abi>=5.0.1->web3)\n",
            "  Downloading parsimonious-0.10.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pycryptodome<4,>=3.6.6 (from eth-hash[pycryptodome]>=0.5.1->web3)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting cytoolz>=0.10.1 (from eth-utils>=5.0.0->web3)\n",
            "  Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython) (0.8.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.14)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4.0->web3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4.0->web3) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from trie>=2.0.0->py-evm) (2.4.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (0.12.1)\n",
            "Requirement already satisfied: regex>=2022.3.15 in /usr/local/lib/python3.12/dist-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading keras_tuner-1.4.8-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading web3-7.14.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_account-0.13.7-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m587.5/587.5 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_solc_x-2.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading eth_tester-0.13.0b1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_evm-0.12.1b1-py3-none-any.whl (798 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m340.3/340.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cached_property-2.0.1-py3-none-any.whl (7.4 kB)\n",
            "Downloading ckzg-2.1.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_abi-5.2.0-py3-none-any.whl (28 kB)\n",
            "Downloading eth_bloom-3.1.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading eth_hash-0.7.1-py3-none-any.whl (8.0 kB)\n",
            "Downloading eth_keyfile-0.8.1-py3-none-any.whl (7.5 kB)\n",
            "Downloading eth_keys-0.7.0-py3-none-any.whl (20 kB)\n",
            "Downloading eth_rlp-2.2.0-py3-none-any.whl (4.4 kB)\n",
            "Downloading eth_typing-5.2.1-py3-none-any.whl (19 kB)\n",
            "Downloading eth_utils-5.3.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hexbytes-1.3.1-py3-none-any.whl (5.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lru_dict-1.4.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (31 kB)\n",
            "Downloading py_ecc-8.0.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyunormalize-17.0.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rlp-4.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading trie-3.1.0-py3-none-any.whl (38 kB)\n",
            "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kt-legacy, ckzg, bitarray, types-requests, pyunormalize, pycryptodome, parsimonious, lru-dict, jedi, hexbytes, eth-typing, eth-hash, cytoolz, cached-property, py-solc-x, eth-utils, eth-bloom, rlp, py-ecc, keras-tuner, eth-keys, eth-abi, trie, eth-rlp, eth-keyfile, py-evm, eth-account, web3, eth-tester\n",
            "Successfully installed bitarray-3.8.0 cached-property-2.0.1 ckzg-2.1.5 cytoolz-1.1.0 eth-abi-5.2.0 eth-account-0.13.7 eth-bloom-3.1.0 eth-hash-0.7.1 eth-keyfile-0.8.1 eth-keys-0.7.0 eth-rlp-2.2.0 eth-tester-0.13.0b1 eth-typing-5.2.1 eth-utils-5.3.1 hexbytes-1.3.1 jedi-0.19.2 keras-tuner-1.4.8 kt-legacy-1.0.5 lru-dict-1.4.1 parsimonious-0.10.0 py-ecc-8.0.0 py-evm-0.12.1b1 py-solc-x-2.0.4 pycryptodome-3.23.0 pyunormalize-17.0.0 rlp-4.1.0 trie-3.1.0 types-requests-2.32.4.20250913 web3-7.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn tensorflow keras-tuner web3 eth-account py-solc-x eth-tester pandas numpy matplotlib seaborn ipython py-evm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yqo013gvwReE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from web3 import Web3\n",
        "from eth_account import Account\n",
        "from solcx import compile_source, install_solc\n",
        "from datetime import datetime\n",
        "import time\n",
        "from eth_tester import EthereumTester, PyEVMBackend\n",
        "from web3.providers.eth_tester import EthereumTesterProvider\n",
        "\n",
        "# Install Solidity compiler\n",
        "install_solc('0.8.0')\n",
        "\n",
        "# Solidity smart contract with Proof of Green Compliance\n",
        "CONTRACT_SOURCE = \"\"\"\n",
        "// SPDX-License-Identifier: MIT\n",
        "pragma solidity ^0.8.0;\n",
        "\n",
        "contract SupplyChain {\n",
        "    struct Order {\n",
        "        string orderId;\n",
        "        string product;\n",
        "        uint256 quantity;\n",
        "        string status;\n",
        "        uint256 timestamp;\n",
        "        uint256 emissions; // in kg CO2 * 100 for precision\n",
        "        address sender;\n",
        "    }\n",
        "\n",
        "    mapping(string => Order) public orders;\n",
        "    uint256 public orderCount;\n",
        "    uint256 public totalEmissions; // in kg CO2 * 100\n",
        "    uint256 public emissionThreshold = 1000; // 10 kg CO2 * 100\n",
        "\n",
        "    event OrderCreated(string orderId, string product, uint256 quantity, uint256 emissions);\n",
        "    event ShipmentUpdated(string orderId, string status, uint256 emissions);\n",
        "    event ReorderTriggered(string orderId, string product, uint256 quantity);\n",
        "\n",
        "    function createOrder(string memory orderId, string memory product, uint256 quantity, uint256 emissions) public {\n",
        "        require(emissions <= emissionThreshold, \"Emissions exceed Proof of Green Compliance threshold\");\n",
        "        require(bytes(orders[orderId].orderId).length == 0, \"Order ID already exists\");\n",
        "\n",
        "        orders[orderId] = Order(orderId, product, quantity, \"Created\", block.timestamp, emissions, msg.sender);\n",
        "        orderCount++;\n",
        "        totalEmissions += emissions;\n",
        "        emit OrderCreated(orderId, product, quantity, emissions);\n",
        "    }\n",
        "\n",
        "    function updateShipment(string memory orderId, string memory status, uint256 emissions) public {\n",
        "        require(bytes(orders[orderId].orderId).length != 0, \"Order does not exist\");\n",
        "        require(emissions <= emissionThreshold, \"Emissions exceed Proof of Green Compliance threshold\");\n",
        "\n",
        "        orders[orderId].status = status;\n",
        "        orders[orderId].emissions += emissions;\n",
        "        totalEmissions += emissions;\n",
        "        emit ShipmentUpdated(orderId, status, emissions);\n",
        "    }\n",
        "\n",
        "    function automateReorder(string memory orderId, string memory product, uint256 currentQuantity, uint256 threshold, string memory newOrderId, uint256 emissions) public {\n",
        "        require(currentQuantity < threshold, \"Current quantity above threshold\");\n",
        "        createOrder(newOrderId, product, threshold - currentQuantity, emissions);\n",
        "        emit ReorderTriggered(newOrderId, product, threshold - currentQuantity);\n",
        "    }\n",
        "\n",
        "    function getOrder(string memory orderId) public view returns (string memory, string memory, uint256, string memory, uint256, uint256, address) {\n",
        "        Order memory order = orders[orderId];\n",
        "        return (order.orderId, order.product, order.quantity, order.status, order.timestamp, order.emissions, order.sender);\n",
        "    }\n",
        "\n",
        "    function getTotalEmissions() public view returns (uint256) {\n",
        "        return totalEmissions;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load and preprocess DataCo dataset\n",
        "def load_data():\n",
        "    try:\n",
        "        file_path = '/content/DataCoSupplyChainDataset.csv'\n",
        "        df = pd.read_csv(file_path, encoding='latin-1')\n",
        "        print(f\"Successfully loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "\n",
        "        df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'], errors='coerce')\n",
        "        if df['order date (DateOrders)'].isna().sum() > 0:\n",
        "            print(f\"Warning: {df['order date (DateOrders)'].isna().sum()} invalid dates set to NaT\")\n",
        "\n",
        "        numerical_cols = ['Order Item Quantity', 'Sales', 'Days for shipping (real)']\n",
        "        for col in numerical_cols:\n",
        "            df[col] = df[col].fillna(df[col].mean())\n",
        "        df.fillna(0, inplace=True)\n",
        "        print(f\"Missing values handled: {numerical_cols} imputed with mean, others with 0\")\n",
        "\n",
        "        # Ensure Product Name is uniformly string\n",
        "        df['Product Name'] = df['Product Name'].astype(str)\n",
        "        # Validate Product Name entries\n",
        "        invalid_entries = df['Product Name'].isna() | (df['Product Name'] == '')\n",
        "        if invalid_entries.any():\n",
        "            print(f\"Warning: {invalid_entries.sum()} invalid Product Name entries replaced with 'Unknown'\")\n",
        "            df.loc[invalid_entries, 'Product Name'] = 'Unknown'\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        df['product'] = le.fit_transform(df['Product Name'])\n",
        "        print(f\"Encoded {len(le.classes_)} unique products\")\n",
        "\n",
        "        required_cols = ['order date (DateOrders)', 'Order Item Quantity', 'Product Name', 'product', 'Sales', 'Days for shipping (real)', 'Shipping Mode']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Filter for high-demand products\n",
        "        df = df[df['Order Item Quantity'] >= 5]\n",
        "        print(f\"Filtered dataset to {df.shape[0]} rows with Order Item Quantity >= 5\")\n",
        "\n",
        "        return df, le\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {file_path} not found. Please upload DataCoSupplyChainDataset.csv to /content/.\")\n",
        "        print(\"Download from: https://data.mendeley.com/datasets/8gx2fvg2k6/5\")\n",
        "        print(\"Use: from google.colab import files; uploaded = files.upload()\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Prepare data for LSTM\n",
        "def prepare_lstm_data(df, product_id, scaler):\n",
        "    product_data = df[df['product'] == product_id][['order date (DateOrders)', 'Order Item Quantity']].set_index('order date (DateOrders)')\n",
        "    product_data = product_data.resample('D').sum().fillna(0)\n",
        "    data = product_data['Order Item Quantity'].values\n",
        "    data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - 30):\n",
        "        X.append(data[i:i+30])\n",
        "        y.append(data[i+30])\n",
        "    return np.array(X), np.array(y), scaler\n",
        "\n",
        "# Train LSTM model\n",
        "def train_lstm(df):\n",
        "    product_id = df['product'].iloc[0]\n",
        "    scaler = MinMaxScaler()\n",
        "    X, y, scaler = prepare_lstm_data(df, product_id, scaler)\n",
        "    if len(X) == 0:\n",
        "        print(\"Error: Insufficient data for LSTM training\")\n",
        "        return None, None, None\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "    inputs = Input(shape=(30, 1))\n",
        "    lstm = LSTM(50, activation='relu')(inputs)\n",
        "    outputs = Dense(1)(lstm)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
        "    y_pred = model.predict(X, verbose=0)\n",
        "    mae = mean_absolute_error(scaler.inverse_transform(y.reshape(-1, 1)), scaler.inverse_transform(y_pred))\n",
        "    rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y.reshape(-1, 1)), scaler.inverse_transform(y_pred)))\n",
        "    print(f\"LSTM MAE: {mae:.2f} units, RMSE: {rmse:.2f} units\")\n",
        "    return model, product_id, scaler\n",
        "\n",
        "# Train Random Forest\n",
        "def train_rf(df):\n",
        "    X = df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']]\n",
        "    y = df['Order Item Quantity'] * 1.5\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    return model, X.columns\n",
        "\n",
        "# Optimize emissions\n",
        "def optimize_emissions(df, product_id):\n",
        "    product_data = df[df['product'] == product_id][['Shipping Mode', 'Order Item Quantity']]\n",
        "    emissions_rates = {'Standard Shipping': 0.2, 'Second Class': 0.25, 'First Class': 0.3, 'Same Day': 0.35}\n",
        "    total_emissions = product_data.groupby('Shipping Mode')['Order Item Quantity'].sum().to_dict()\n",
        "    for mode in total_emissions:\n",
        "        total_emissions[mode] *= emissions_rates.get(mode, 0.2)\n",
        "    recommended_mode = min(emissions_rates, key=lambda x: emissions_rates[x])\n",
        "    return recommended_mode, total_emissions\n",
        "\n",
        "# Setup mock blockchain with eth-tester\n",
        "def setup_mock_blockchain():\n",
        "    try:\n",
        "        print(\"Setting up mock blockchain with eth-tester...\")\n",
        "        tester = EthereumTester(backend=PyEVMBackend())\n",
        "        w3 = Web3(EthereumTesterProvider(tester))\n",
        "        account = tester.get_accounts()[0]\n",
        "\n",
        "        # Compile Solidity contract\n",
        "        compiled_sol = compile_source(CONTRACT_SOURCE, output_values=['abi', 'bin'])\n",
        "        contract_interface = compiled_sol['<stdin>:SupplyChain']\n",
        "        abi = contract_interface['abi']\n",
        "        bytecode = contract_interface['bin']\n",
        "\n",
        "        # Deploy contract\n",
        "        contract = w3.eth.contract(abi=abi, bytecode=bytecode)\n",
        "        tx_hash = contract.constructor().transact({'from': account})\n",
        "        tx_receipt = w3.eth.get_transaction_receipt(tx_hash)\n",
        "        contract_address = tx_receipt.contractAddress\n",
        "        contract_instance = w3.eth.contract(address=contract_address, abi=abi)\n",
        "\n",
        "        return w3, contract_instance, account\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up mock blockchain: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"=== Decentralized Autonomous Supply Chain Organization (DASCO) ===\")\n",
        "    print(\"Demonstrating ML and Ethereum Blockchain with Proof of Green Compliance\\n\")\n",
        "\n",
        "    # Setup blockchain\n",
        "    w3, contract, account = setup_mock_blockchain()\n",
        "    if w3 is None:\n",
        "        print(\"Failed to setup blockchain. Exiting...\")\n",
        "        return\n",
        "    print(f\"Mock Blockchain: Contract deployed at {contract.address}\")\n",
        "\n",
        "    # Load dataset\n",
        "    df, le = load_data()\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Train ML models\n",
        "    lstm_model, product_id, scaler = train_lstm(df)\n",
        "    rf_model, rf_columns = train_rf(df)\n",
        "\n",
        "    if lstm_model is None or product_id is None:\n",
        "        return\n",
        "\n",
        "    # ML predictions\n",
        "    X, _, _ = prepare_lstm_data(df, product_id, scaler)\n",
        "    if len(X) == 0:\n",
        "        print(\"Error: No data available for prediction\")\n",
        "        return\n",
        "    X = X[-1].reshape((1, 30, 1))\n",
        "    demand_scaled = lstm_model.predict(X, verbose=0)[0][0]\n",
        "    demand = scaler.inverse_transform([[demand_scaled]])[0][0]\n",
        "\n",
        "    sample = pd.DataFrame([df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']].iloc[0].values], columns=rf_columns)\n",
        "    optimal_stock = rf_model.predict(sample)[0]\n",
        "\n",
        "    recommended_mode, emissions_data = optimize_emissions(df, product_id)\n",
        "\n",
        "    # Blockchain transactions\n",
        "    product_name = le.inverse_transform([product_id])[0]\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    current_quantity = 2  # Adjusted to trigger reorder\n",
        "    threshold = int(optimal_stock)\n",
        "\n",
        "    try:\n",
        "        # Create order\n",
        "        order_id = 'TX001'\n",
        "        quantity = current_quantity\n",
        "        emissions = int(quantity * 0.2 * 100)  # kg CO2 * 100\n",
        "        tx_hash = contract.functions.createOrder(order_id, product_name, quantity, emissions).transact({'from': account, 'gas': 1000000})\n",
        "        receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n",
        "        print(f\"Blockchain: Created order {order_id} for {quantity} units of {product_name} (Emissions: {emissions/100:.2f} kg CO2, Tx Hash: {tx_hash.hex()})\")\n",
        "\n",
        "        # Update shipment\n",
        "        emissions_update = int(quantity * 0.1 * 100)\n",
        "        tx_hash = contract.functions.updateShipment(order_id, \"In Transit\", emissions_update).transact({'from': account, 'gas': 1000000})\n",
        "        receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n",
        "        print(f\"Blockchain: Updated order {order_id} to 'In Transit' (Emissions: {emissions_update/100:.2f} kg CO2, Tx Hash: {tx_hash.hex()})\")\n",
        "\n",
        "        # Automate reorder\n",
        "        new_order_id = f\"REORDER_TX001_{int(time.time())}\"\n",
        "        emissions_reorder = int((threshold - current_quantity) * 0.1 * 100)\n",
        "        tx_hash = contract.functions.automateReorder(order_id, product_name, current_quantity, threshold, new_order_id, emissions_reorder).transact({'from': account, 'gas': 1000000})\n",
        "        receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n",
        "        print(f\"Blockchain: Triggered reorder {new_order_id} for {threshold - current_quantity} units (Emissions: {emissions_reorder/100:.2f} kg CO2, Tx Hash: {tx_hash.hex()})\")\n",
        "\n",
        "        # Retrieve transactions\n",
        "        order_count = contract.functions.orderCount().call()\n",
        "        total_emissions = contract.functions.getTotalEmissions().call() / 100\n",
        "        orders = []\n",
        "        for order_id_to_fetch in ['TX001', new_order_id]:\n",
        "            try:\n",
        "                order_data = contract.functions.getOrder(order_id_to_fetch).call()\n",
        "                orders.append({\n",
        "                    'orderId': order_data[0],\n",
        "                    'product': order_data[1],\n",
        "                    'quantity': order_data[2],\n",
        "                    'status': order_data[3],\n",
        "                    'timestamp': datetime.fromtimestamp(order_data[4]).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'emissions': order_data[5] / 100,\n",
        "                    'sender': order_data[6]\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Blockchain Error: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n1. ML Demand Prediction:\")\n",
        "    print(f\"   Product: {product_name}\")\n",
        "    print(f\"   Predicted Demand: {round(float(demand), 2)} units\")\n",
        "    print(f\"   Date: {timestamp.split()[0]}\\n\")\n",
        "\n",
        "    print(\"2. Inventory Optimization:\")\n",
        "    print(f\"   Product: {product_name}\")\n",
        "    print(f\"   Optimal Stock: {round(float(optimal_stock), 2)} units\")\n",
        "    print(f\"   Current Stock: {current_quantity} units\")\n",
        "    print(f\"   Reorder Threshold: {threshold} units\\n\")\n",
        "\n",
        "    print(\"3. Sustainability Analysis:\")\n",
        "    print(f\"   Recommended Shipping Mode: {recommended_mode} (lowest emissions)\")\n",
        "    print(f\"   Current Emissions by Mode: {emissions_data}\")\n",
        "    print(f\"   Total Emissions Recorded on Blockchain: {total_emissions:.2f} kg CO2\\n\")\n",
        "\n",
        "    print(\"4. Blockchain Transactions (Proof of Green Compliance):\")\n",
        "    for order in orders:\n",
        "        print(f\"   ID: {order['orderId']}, Product: {order['product']}, Quantity: {order['quantity']}, \"\n",
        "              f\"Status: {order['status']}, Emissions: {order['emissions']:.2f} kg CO2, Timestamp: {order['timestamp']}, Sender: {order['sender']}\")\n",
        "\n",
        "    print(\"\\n=== Summary ===\")\n",
        "    print(\"This prototype demonstrates DASCO's integration of ML and Ethereum blockchain with Proof of Green Compliance, ensuring transparent, automated, and sustainable supply chain management.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjCbQvNhvJN3",
        "outputId": "9b28fb3b-aaac-4d89-8d20-4ecd0b636f53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Decentralized Autonomous Supply Chain Organization (DASCO) ===\n",
            "Demonstrating ML and Ethereum Blockchain with Proof of Green Compliance\n",
            "\n",
            "Setting up mock blockchain with eth-tester...\n",
            "Mock Blockchain: Contract deployed at 0xF2E246BB76DF876Cef8b38ae84130F4F55De395b\n",
            "Successfully loaded dataset with 47560 rows and 53 columns\n",
            "Missing values handled: ['Order Item Quantity', 'Sales', 'Days for shipping (real)'] imputed with mean, others with 0\n",
            "Encoded 98 unique products\n",
            "Filtered dataset to 6959 rows with Order Item Quantity >= 5\n",
            "LSTM MAE: 5.36 units, RMSE: 6.72 units\n",
            "Blockchain: Created order TX001 for 2 units of Nike Men's Dri-FIT Victory Golf Polo (Emissions: 0.40 kg CO2, Tx Hash: 8c94a2c4a8f975e2c2124a3de7657ae31b73963b9d11b2f09a6a6ca33a0027c4)\n",
            "Blockchain: Updated order TX001 to 'In Transit' (Emissions: 0.20 kg CO2, Tx Hash: 327d1e6d4a38a4f952f2414974f149526308fd40407483d38e6878b4a154661b)\n",
            "Blockchain: Triggered reorder REORDER_TX001_1763885686 for 5 units (Emissions: 0.50 kg CO2, Tx Hash: 08328186e44a4866c9297a6512e65c735c10bd91881418565136e1c78d3663ea)\n",
            "\n",
            "1. ML Demand Prediction:\n",
            "   Product: Nike Men's Dri-FIT Victory Golf Polo\n",
            "   Predicted Demand: 8.87 units\n",
            "   Date: 2025-11-23\n",
            "\n",
            "2. Inventory Optimization:\n",
            "   Product: Nike Men's Dri-FIT Victory Golf Polo\n",
            "   Optimal Stock: 7.5 units\n",
            "   Current Stock: 2 units\n",
            "   Reorder Threshold: 7 units\n",
            "\n",
            "3. Sustainability Analysis:\n",
            "   Recommended Shipping Mode: Standard Shipping (lowest emissions)\n",
            "   Current Emissions by Mode: {'First Class': 442.5, 'Same Day': 125.99999999999999, 'Second Class': 361.25, 'Standard Class': 1099.0}\n",
            "   Total Emissions Recorded on Blockchain: 1.10 kg CO2\n",
            "\n",
            "4. Blockchain Transactions (Proof of Green Compliance):\n",
            "   ID: TX001, Product: Nike Men's Dri-FIT Victory Golf Polo, Quantity: 2, Status: In Transit, Emissions: 0.60 kg CO2, Timestamp: 2025-11-23 08:14:29, Sender: 0x7E5F4552091A69125d5DfCb7b8C2659029395Bdf\n",
            "   ID: REORDER_TX001_1763885686, Product: Nike Men's Dri-FIT Victory Golf Polo, Quantity: 5, Status: Created, Emissions: 0.50 kg CO2, Timestamp: 2025-11-23 08:14:47, Sender: 0x7E5F4552091A69125d5DfCb7b8C2659029395Bdf\n",
            "\n",
            "=== Summary ===\n",
            "This prototype demonstrates DASCO's integration of ML and Ethereum blockchain with Proof of Green Compliance, ensuring transparent, automated, and sustainable supply chain management.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DASCO prototype \u2014 improved single-file script\\n",
        "# Features:\\n",
        "# - Robust data loading and validation\\n",
        "# - LSTM forecasting with train/validation, scaler handling\\n",
        "# - RandomForest predicting order quantity (train/test + scaler)\\n",
        "# - Correct emissions optimization (min total emissions)\\n",
        "# - Robust eth-tester mock blockchain setup & solidity compile/deploy\\n",
        "# - Clear prints and error handling\\n",
        "\\n",
        "import os\\n",
        "import time\\n",
        "from datetime import datetime\\n",
        "\\n",
        "import numpy as np\\n",
        "import pandas as pd\\n",
        "from sklearn.ensemble import RandomForestRegressor\\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\\n",
        "from sklearn.model_selection import train_test_split\\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\\n",
        "\\n",
        "from tensorflow.keras.models import Model\\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\\n",
        "from tensorflow.keras.callbacks import EarlyStopping\\n",
        "\\n",
        "# blockchain imports\\n",
        "from web3 import Web3\\n",
        "from solcx import compile_source, install_solc\\n",
        "from eth_tester import EthereumTester, PyEVMBackend\\n",
        "from web3.providers.eth_tester import EthereumTesterProvider\\n",
        "\\n",
        "from sklearn.ensemble import IsolationForest\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "\\n",
        "\\n",
        "# Solidity contract (unchanged structure except comments)\\n",
        "CONTRACT_SOURCE = \"\"\"\\n",
        "// SPDX-License-Identifier: MIT\\n",
        "pragma solidity ^0.8.0;\\n",
        "\\n",
        "contract SupplyChain {\\n",
        "    struct Order {\\n",
        "        string orderId;\\n",
        "        string product;\\n",
        "        uint256 quantity;\\n",
        "        string status;\\n",
        "        uint256 timestamp;\\n",
        "        uint256 emissions; // in kg CO2 * 100 for precision\\n",
        "        address sender;\\n",
        "    }\\n",
        "\\n",
        "    mapping(string => Order) public orders;\\n",
        "    uint256 public orderCount;\\n",
        "    uint256 public totalEmissions; // in kg CO2 * 100\\n",
        "    uint256 public emissionThreshold = 1000; // 10 kg CO2 * 100\\n",
        "\\n",
        "    event OrderCreated(string orderId, string product, uint256 quantity, uint256 emissions);\\n",
        "    event ShipmentUpdated(string orderId, string status, uint256 emissions);\\n",
        "    event ReorderTriggered(string orderId, string product, uint256 quantity);\\n",
        "\\n",
        "    function createOrder(string memory orderId, string memory product, uint256 quantity, uint256 emissions) public {\\n",
        "        require(emissions <= emissionThreshold, \"Emissions exceed Proof of Green Compliance threshold\");\\n",
        "        require(bytes(orders[orderId].orderId).length == 0, \"Order ID already exists\");\\n",
        "\\n",
        "        orders[orderId] = Order(orderId, product, quantity, \"Created\", block.timestamp, emissions, msg.sender);\\n",
        "        orderCount++;\\n",
        "        totalEmissions += emissions;\\n",
        "        emit OrderCreated(orderId, product, quantity, emissions);\\n",
        "    }\\n",
        "\\n",
        "    function updateShipment(string memory orderId, string memory status, uint256 emissions) public {\\n",
        "        require(bytes(orders[orderId].orderId).length != 0, \"Order does not exist\");\\n",
        "        require(emissions <= emissionThreshold, \"Emissions exceed Proof of Green Compliance threshold\");\\n",
        "\\n",
        "        orders[orderId].status = status;\\n",
        "        orders[orderId].emissions += emissions;\\n",
        "        totalEmissions += emissions;\\n",
        "        emit ShipmentUpdated(orderId, status, emissions);\\n",
        "    }\\n",
        "\\n",
        "    function automateReorder(string memory orderId, string memory product, uint256 currentQuantity, uint256 threshold, string memory newOrderId, uint256 emissions) public {\\n",
        "        require(currentQuantity < threshold, \"Current quantity above threshold\");\\n",
        "        createOrder(newOrderId, product, threshold - currentQuantity, emissions);\\n",
        "        emit ReorderTriggered(newOrderId, product, threshold - currentQuantity);\\n",
        "    }\\n",
        "\\n",
        "    function getOrder(string memory orderId) public view returns (string memory, string memory, uint256, string memory, uint256, uint256, address) {\\n",
        "        Order memory order = orders[orderId];\\n",
        "        return (order.orderId, order.product, order.quantity, order.status, order.timestamp, order.emissions, order.sender);\\n",
        "    }\\n",
        "\\n",
        "    function getTotalEmissions() public view returns (uint256) {\\n",
        "        return totalEmissions;\\n",
        "    }\\n",
        "}\\n",
        "\"\"\"\\n",
        "\\n",
        "# -------------------------\\n",
        "# Data loading & preprocessing\\n",
        "# -------------------------\\n",
        "def load_data(file_path='/content/DataCoSupplyChainDataset.csv', min_qty_filter=None):\\n",
        "    \"\"\"\\n",
        "    Load the DataCo dataset with safer checks.\\n",
        "    min_qty_filter: if provided, filter Order Item Quantity >= min_qty_filter (optional).\\n",
        "    Returns: df (pandas.DataFrame), label_encoder (LabelEncoder for product names)\\n",
        "    \"\"\"\\n",
        "    if not os.path.exists(file_path):\\n",
        "        print(f\"Error: dataset not found at {file_path}. Please upload the file or change file_path.\")\\n",
        "        return None, None\\n",
        "\\n",
        "    try:\\n",
        "        df = pd.read_csv(file_path, encoding='latin-1')\\n",
        "        # normalize column names (strip whitespace)\\n",
        "        df.columns = [c.strip() for c in df.columns]\\n",
        "\\n",
        "        # required columns (adapt if your CSV has slightly different names)\\n",
        "        required_cols = ['order date (DateOrders)', 'Order Item Quantity', 'Product Name', 'Sales', 'Days for shipping (real)', 'Shipping Mode']\\n",
        "        missing_cols = [c for c in required_cols if c not in df.columns]\\n",
        "        if missing_cols:\\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\\n",
        "\\n",
        "        # parse dates\\n",
        "        df['order date (DateOrders)'] = pd.to_datetime(df['order date (DateOrders)'], errors='coerce')\\n",
        "        if df['order date (DateOrders)'].isna().sum() > 0:\\n",
        "            print(f\"Warning: {df['order date (DateOrders)'].isna().sum()} invalid dates set to NaT\")\\n",
        "\\n",
        "        # numeric conversions & imputation for numeric columns only\\n",
        "        num_cols = ['Order Item Quantity', 'Sales', 'Days for shipping (real)']\\n",
        "        for col in num_cols:\\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\\n",
        "            mean_val = df[col].mean()\\n",
        "            df[col].fillna(mean_val, inplace=True)\\n",
        "\\n",
        "        # Product Name handling\\n",
        "        df['Product Name'] = df['Product Name'].astype(str).fillna('Unknown')\\n",
        "        invalid_entries = df['Product Name'].isna() | (df['Product Name'] == '')\\n",
        "        if invalid_entries.any():\\n",
        "            df.loc[invalid_entries, 'Product Name'] = 'Unknown'\\n",
        "\\n",
        "        # encode product\\n",
        "        le = LabelEncoder()\\n",
        "        df['product'] = le.fit_transform(df['Product Name'])\\n",
        "\\n",
        "        # optional filter by minimum order item quantity\\n",
        "        if min_qty_filter is not None:\\n",
        "            before = len(df)\\n",
        "            df = df[df['Order Item Quantity'] >= min_qty_filter].copy()\\n",
        "            after = len(df)\\n",
        "            print(f\"Applied min_qty_filter={min_qty_filter}: rows {before} -> {after}\")\\n",
        "\\n",
        "        print(f\"Loaded dataset with shape: {df.shape} and {len(le.classes_)} unique products\")\\n",
        "        return df, le\\n",
        "\\n",
        "    except Exception as e:\\n",
        "        print(\"Error loading data:\", e)\\n",
        "        return None, None\\n",
        "\\n",
        "# -------------------------\\n",
        "# LSTM time-series preparation and training\\n",
        "# -------------------------\\n",
        "def prepare_lstm_data(df, product_id, scaler=None, lookback=30):\\n",
        "    \"\"\"\\n",
        "    Prepare daily-resampled time series for a particular product.\\n",
        "    Returns X (num_samples, lookback), y (num_samples,), scaler (fitted)\\n",
        "    \"\"\"\\n",
        "    prod_df = df[df['product'] == product_id][['order date (DateOrders)', 'Order Item Quantity']].copy()\\n",
        "    if prod_df.empty:\\n",
        "        return np.array([]), np.array([]), scaler\\n",
        "\\n",
        "    prod_df = prod_df.set_index('order date (DateOrders)').sort_index()\\n",
        "    # resample to daily frequency and sum quantities per day\\n",
        "    prod_daily = prod_df.resample('D').sum().fillna(0)\\n",
        "    data = prod_daily['Order Item Quantity'].values.reshape(-1, 1)\\n",
        "\\n",
        "    if scaler is None:\\n",
        "        scaler = MinMaxScaler()\\n",
        "    data_scaled = scaler.fit_transform(data)\\n",
        "\\n",
        "    X, y = [], []\\n",
        "    for i in range(len(data_scaled) - lookback):\\n",
        "        X.append(data_scaled[i:i + lookback, 0])\\n",
        "        y.append(data_scaled[i + lookback, 0])\\n",
        "\\n",
        "    return np.array(X), np.array(y), scaler\\n",
        "\\n",
        "def train_lstm(df, product_id=None, lookback=30, epochs=30):\\n",
        "    \"\"\"\\n",
        "    Train an LSTM on the product time series.\\n",
        "    If product_id is None, chooses product with most records.\\n",
        "    Returns model, product_id, scaler\\n",
        "    \"\"\"\\n",
        "    if product_id is None:\\n",
        "        product_id = df['product'].value_counts().idxmax()\\n",
        "        print(f\"No product_id provided. Using most frequent product: {product_id}\")\\n",
        "\\n",
        "    scaler = MinMaxScaler()\\n",
        "    X, y, scaler = prepare_lstm_data(df, product_id, scaler=scaler, lookback=lookback)\\n",
        "    if len(X) == 0:\\n",
        "        print(\"Insufficient data for LSTM training\")\\n",
        "        return None, None, None\\n",
        "\\n",
        "    # train/validation split (time-order preserved)\\n",
        "    split = int(len(X) * 0.8)\\n",
        "    X_train, X_val = X[:split], X[split:]\\n",
        "    y_train, y_val = y[:split], y[split:]\\n",
        "\\n",
        "    # reshape for LSTM\\n",
        "    X_train = X_train.reshape((X_train.shape[0], lookback, 1))\\n",
        "    X_val = X_val.reshape((X_val.shape[0], lookback, 1))\\n",
        "\\n",
        "    inputs = Input(shape=(lookback, 1))\\n",
        "    lstm = LSTM(50)(inputs)  # default tanh internal activations\\n",
        "    outputs = Dense(1)(lstm)\\n",
        "    model = Model(inputs=inputs, outputs=outputs)\\n",
        "    model.compile(optimizer='adam', loss='mse')\\n",
        "\\n",
        "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\\n",
        "    print(f\"Training LSTM for product {product_id} \u2014 epochs={epochs}, lookback={lookback}\")\\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_val, y_val), callbacks=[es], verbose=1)\\n",
        "\\n",
        "    # validation metrics\\n",
        "    if len(X_val) > 0:\\n",
        "        y_val_pred = model.predict(X_val)\\n",
        "        y_val_inv = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()\\n",
        "        y_val_pred_inv = scaler.inverse_transform(y_val_pred).flatten()\\n",
        "        val_mae = mean_absolute_error(y_val_inv, y_val_pred_inv)\\n",
        "        val_rmse = np.sqrt(mean_squared_error(y_val_inv, y_val_pred_inv))\\n",
        "        print(f\"LSTM Validation MAE: {val_mae:.2f}, RMSE: {val_rmse:.2f}\")\\n",
        "    else:\\n",
        "        print(\"No validation split available to report metrics.\")\\n",
        "\\n",
        "    return model, product_id, scaler\\n",
        "\\n",
        "# -------------------------\\n",
        "# Random Forest training (predicting Order Item Quantity)\\n",
        "# -------------------------\\n",
        "def train_rf(df):\\n",
        "    \"\"\"\\n",
        "    Train RF to predict Order Item Quantity using Sales and Days for shipping (real).\\n",
        "    Returns model, feature_columns (list), scaler\\n",
        "    \"\"\"\\n",
        "    X = df[['Sales', 'Days for shipping (real)']].copy()\\n",
        "    y = df['Order Item Quantity'].copy().astype(float)\\n",
        "\\n",
        "    # simple train/test split\\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n",
        "\\n",
        "    scaler = StandardScaler()\\n",
        "    X_train_sc = scaler.fit_transform(X_train)\\n",
        "    X_test_sc = scaler.transform(X_test)\\n",
        "\\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\\n",
        "    model.fit(X_train_sc, y_train)\\n",
        "\\n",
        "    y_pred = model.predict(X_test_sc)\\n",
        "    print(f\"RF Test MAE: {mean_absolute_error(y_test, y_pred):.2f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}, R2: {r2_score(y_test, y_pred):.3f}\")\\n",
        "\\n",
        "    return model, X.columns.tolist(), scaler\\n",
        "\\n",
        "# -------------------------\\n",
        "# Emissions optimization\\n",
        "# -------------------------\\n",
        "def optimize_emissions(df, product_id):\\n",
        "    \"\"\"\\n",
        "    Compute total emissions per shipping mode for a product and recommend the mode that\\n",
        "    minimizes total emissions (qty * per-unit-rate).\\n",
        "    Returns recommended_mode, dict(total_emissions_by_mode)\\n",
        "    \"\"\"\\n",
        "    product_data = df[df['product'] == product_id][['Shipping Mode', 'Order Item Quantity']].copy()\\n",
        "    # default per-unit emissions (kg CO2 per unit)\\n",
        "    emissions_rates = {'Standard Shipping': 0.2, 'Second Class': 0.25, 'First Class': 0.3, 'Same Day': 0.35}\\n",
        "\\n",
        "    totals = product_data.groupby('Shipping Mode')['Order Item Quantity'].sum().to_dict()\\n",
        "    total_emissions_by_mode = {mode: totals.get(mode, 0.0) * emissions_rates.get(mode, 0.2) for mode in emissions_rates}\\n",
        "\\n",
        "    recommended_mode = min(total_emissions_by_mode, key=lambda m: total_emissions_by_mode[m])\\n",
        "    return recommended_mode, total_emissions_by_mode\\n",
        "\\n",
        "# -------------------------\\n",
        "# Mock blockchain setup (eth-tester) with robust deploy/tx handling\\n",
        "# -------------------------\\n",
        "def setup_mock_blockchain():\\n",
        "    \"\"\"\\n",
        "    Sets up an eth-tester blockchain, compiles the solidity contract, deploys and returns:\\n",
        "    w3 (Web3), contract_instance (Contract), account (deployer address)\\n",
        "    \"\"\"\\n",
        "    try:\\n",
        "        print(\"Attempting to install solc 0.8.0 (if not already installed)...\")\\n",
        "        try:\\n",
        "            install_solc('0.8.0')\\n",
        "        except Exception as e:\\n",
        "            print(\"Warning: install_solc failed or network not available. If compile fails, install a local solc 0.8.x manually. Error:\", e)\\n",
        "\\n",
        "        print(\"Setting up eth-tester backend...\")\\n",
        "        tester = EthereumTester(backend=PyEVMBackend())\\n",
        "        w3 = Web3(EthereumTesterProvider(tester))\\n",
        "        account = tester.get_accounts()[0]\\n",
        "        w3.eth.default_account = account\\n",
        "\\n",
        "        # compile\\n",
        "        compiled = compile_source(CONTRACT_SOURCE, output_values=['abi', 'bin'])\\n",
        "        contract_key = next(iter(compiled.keys()))\\n",
        "        contract_interface = compiled[contract_key]\\n",
        "        abi = contract_interface['abi']\\n",
        "        bytecode = contract_interface['bin']\\n",
        "\\n",
        "        # deploy (wait for receipt)\\n",
        "        contract_factory = w3.eth.contract(abi=abi, bytecode=bytecode)\\n",
        "        tx_hash = contract_factory.constructor().transact()\\n",
        "        tx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\\n",
        "        contract_address = tx_receipt.contractAddress\\n",
        "        contract_instance = w3.eth.contract(address=contract_address, abi=abi)\\n",
        "\\n",
        "        print(f\"Deployed contract at address: {contract_address}\")\\n",
        "        return w3, contract_instance, account\\n",
        "\\n",
        "    except Exception as e:\\n",
        "        print(\"Error setting up mock blockchain:\", e)\\n",
        "        return None, None, None\\n",
        "\\n",
        "# -------------------------\\n",
        "# Main execution\\n",
        "# -------------------------\\n",
        "\\n",
        "class AnomalyDetector:\\n",
        "    \"\"\"NEW FEATURE 1: Real-time Anomaly Detection using Isolation Forest\"\"\"\\n",
        "    def __init__(self, contamination=0.1):\\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\\n",
        "        self.scaler = StandardScaler()\\n",
        "\\n",
        "    def train(self, df):\\n",
        "        features = df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']].copy()\\n",
        "        features_scaled = self.scaler.fit_transform(features)\\n",
        "        self.model.fit(features_scaled)\\n",
        "        print(\"\u2713 Anomaly Detector trained\")\\n",
        "\\n",
        "    def detect(self, df):\\n",
        "        features = df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']].copy()\\n",
        "        features_scaled = self.scaler.transform(features)\\n",
        "        predictions = self.model.predict(features_scaled)\\n",
        "        anomaly_scores = self.model.score_samples(features_scaled)\\n",
        "\\n",
        "        df['is_anomaly'] = predictions == -1\\n",
        "        df['anomaly_score'] = anomaly_scores\\n",
        "\\n",
        "        anomaly_count = df['is_anomaly'].sum()\\n",
        "        print(f\"\u2713 Detected {anomaly_count} anomalies ({anomaly_count/len(df)*100:.2f}%)\")\\n",
        "        return df\\n",
        "\\n",
        "\\n",
        "class PortfolioOptimizer:\\n",
        "    \"\"\"NEW FEATURE 2: Multi-Product Portfolio Optimization\"\"\"\\n",
        "    def __init__(self, df, le):\\n",
        "        self.df = df\\n",
        "        self.le = le\\n",
        "\\n",
        "    def optimize_portfolio(self, top_n=5):\\n",
        "        print(f\"\\n\ud83d\udd0d Analyzing top {top_n} products for portfolio optimization...\")\\n",
        "\\n",
        "        # Calculate metrics per product\\n",
        "        product_stats = self.df.groupby('product').agg({\\n",
        "            'Order Item Quantity': ['sum', 'mean', 'std'],\\n",
        "            'Sales': 'sum',\\n",
        "            'Days for shipping (real)': 'mean'\\n",
        "        }).reset_index()\\n",
        "\\n",
        "        product_stats.columns = ['product', 'total_qty', 'avg_qty', 'std_qty', 'total_sales', 'avg_shipping_days']\\n",
        "        product_stats['revenue_per_unit'] = product_stats['total_sales'] / product_stats['total_qty']\\n",
        "        product_stats['consistency_score'] = 1 / (1 + product_stats['std_qty'])\\n",
        "        product_stats['efficiency_score'] = 1 / (1 + product_stats['avg_shipping_days'])\\n",
        "\\n",
        "        # Composite score\\n",
        "        product_stats['portfolio_score'] = (\\n",
        "            0.4 * product_stats['revenue_per_unit'] / product_stats['revenue_per_unit'].max() +\\n",
        "            0.3 * product_stats['consistency_score'] +\\n",
        "            0.3 * product_stats['efficiency_score']\\n",
        "        )\\n",
        "\\n",
        "        top_products = product_stats.nlargest(top_n, 'portfolio_score')\\n",
        "\\n",
        "        results = []\\n",
        "        for _, row in top_products.iterrows():\\n",
        "            product_name = self.le.inverse_transform([int(row['product'])])[0]\\n",
        "            results.append({\\n",
        "                'product_id': int(row['product']),\\n",
        "                'product_name': product_name,\\n",
        "                'portfolio_score': row['portfolio_score'],\\n",
        "                'total_sales': row['total_sales'],\\n",
        "                'avg_qty': row['avg_qty']\\n",
        "            })\\n",
        "\\n",
        "        print(f\"\u2713 Portfolio optimized with {len(results)} products\")\\n",
        "        return results\\n",
        "\\n",
        "\\n",
        "class DemandCorrelationAnalyzer:\\n",
        "    \"\"\"Feature 3: Cross-Product Demand Correlation Analysis\"\"\"\\n",
        "    def __init__(self, df):\\n",
        "        self.df = df\\n",
        "\\n",
        "    def analyze_correlations(self, top_n=10):\\n",
        "        print(f\"  Analyzing correlations for top {top_n} products...\")\\n",
        "\\n",
        "        top_products = self.df['product'].value_counts().head(top_n).index\\n",
        "\\n",
        "        daily_demand = self.df[self.df['product'].isin(top_products)].pivot_table(\\n",
        "            index='order date (DateOrders)',\\n",
        "            columns='product',\\n",
        "            values='Order Item Quantity',\\n",
        "            aggfunc='sum'\\n",
        "        ).fillna(0)\\n",
        "\\n",
        "        correlation_matrix = daily_demand.corr()\\n",
        "\\n",
        "        correlations = []\\n",
        "        for i in range(len(correlation_matrix)):\\n",
        "            for j in range(i+1, len(correlation_matrix)):\\n",
        "                correlations.append({\\n",
        "                    'product1': correlation_matrix.index[i],\\n",
        "                    'product2': correlation_matrix.columns[j],\\n",
        "                    'correlation': correlation_matrix.iloc[i, j]\\n",
        "                })\\n",
        "\\n",
        "        correlations_df = pd.DataFrame(correlations)\\n",
        "        correlations_df = correlations_df.sort_values('correlation', ascending=False)\\n",
        "\\n",
        "        print(f\"  \u2713 Found {len(correlations_df)} product correlations\")\\n",
        "        return correlations_df.head(5), correlation_matrix\\n",
        "\\n",
        "\\n",
        "\\n",
        "class VisualizationDashboard:\\n",
        "    \"\"\"Feature 6: Advanced Visualization Dashboard\"\"\"\\n",
        "    @staticmethod\\n",
        "    def create_dashboard(df, portfolio_results, anomaly_df):\\n",
        "        print(\"  Creating dashboard...\")\\n",
        "\\n",
        "        fig = plt.figure(figsize=(16, 10))\\n",
        "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\\n",
        "\\n",
        "        # 1. Order Quantity Distribution\\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\\n",
        "        ax1.hist(df['Order Item Quantity'], bins=50, color='skyblue', edgecolor='black')\\n",
        "        ax1.set_title('Order Quantity Distribution', fontweight='bold')\\n",
        "        ax1.set_xlabel('Quantity')\\n",
        "        ax1.set_ylabel('Frequency')\\n",
        "\\n",
        "        # 2. Top Products by Sales\\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\\n",
        "        top_sales = df.groupby('Product Name')['Sales'].sum().nlargest(8)\\n",
        "        ax2.barh(range(len(top_sales)), top_sales.values, color='coral')\\n",
        "        ax2.set_yticks(range(len(top_sales)))\\n",
        "        ax2.set_yticklabels([name[:25] + '...' if len(name) > 25 else name for name in top_sales.index], fontsize=8)\\n",
        "        ax2.set_title('Top 8 Products by Sales', fontweight='bold')\\n",
        "        ax2.set_xlabel('Total Sales ($)')\\n",
        "\\n",
        "        # 3. Shipping Mode Pie Chart\\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\\n",
        "        shipping_dist = df['Shipping Mode'].value_counts()\\n",
        "        colors = plt.cm.Set3(range(len(shipping_dist)))\\n",
        "        ax3.pie(shipping_dist.values, labels=shipping_dist.index, autopct='%1.1f%%',\\n",
        "               startangle=90, colors=colors)\\n",
        "        ax3.set_title('Shipping Mode Distribution', fontweight='bold')\\n",
        "\\n",
        "        # 4. Anomaly Detection Scatter\\n",
        "        ax4 = fig.add_subplot(gs[1, 0])\\n",
        "        if 'is_anomaly' in anomaly_df.columns:\\n",
        "            normal = anomaly_df[anomaly_df['is_anomaly'] == False]\\n",
        "            anomaly = anomaly_df[anomaly_df['is_anomaly'] == True]\\n",
        "            ax4.scatter(normal['Sales'], normal['Order Item Quantity'],\\n",
        "                       alpha=0.3, s=5, label='Normal', color='blue')\\n",
        "            ax4.scatter(anomaly['Sales'], anomaly['Order Item Quantity'],\\n",
        "                       alpha=0.8, s=30, label='Anomaly', color='red', marker='x')\\n",
        "            ax4.set_title('Anomaly Detection Results', fontweight='bold')\\n",
        "            ax4.set_xlabel('Sales ($)')\\n",
        "            ax4.set_ylabel('Order Quantity')\\n",
        "            ax4.legend()\\n",
        "\\n",
        "        # 5. Portfolio Scores\\n",
        "        ax5 = fig.add_subplot(gs[1, 1])\\n",
        "        if portfolio_results:\\n",
        "            portfolio_df = pd.DataFrame(portfolio_results)\\n",
        "            colors_portfolio = plt.cm.Greens(np.linspace(0.4, 0.8, len(portfolio_df)))\\n",
        "            ax5.barh(range(len(portfolio_df)), portfolio_df['portfolio_score'], color=colors_portfolio)\\n",
        "            ax5.set_yticks(range(len(portfolio_df)))\\n",
        "            ax5.set_yticklabels([name[:20] + '...' if len(name) > 20 else name\\n",
        "                                for name in portfolio_df['product_name']], fontsize=8)\\n",
        "            ax5.set_title('Portfolio Optimization Scores', fontweight='bold')\\n",
        "            ax5.set_xlabel('Score')\\n",
        "\\n",
        "        # 6. Emissions by Mode\\n",
        "        ax6 = fig.add_subplot(gs[1, 2])\\n",
        "        emissions_rates = {'Standard Shipping': 0.2, 'Second Class': 0.25,\\n",
        "                          'First Class': 0.3, 'Same Day': 0.35}\\n",
        "        emissions_by_mode = {}\\n",
        "        for mode, rate in emissions_rates.items():\\n",
        "            qty = df[df['Shipping Mode'] == mode]['Order Item Quantity'].sum()\\n",
        "            emissions_by_mode[mode] = qty * rate\\n",
        "\\n",
        "        ax6.bar(range(len(emissions_by_mode)), list(emissions_by_mode.values()),\\n",
        "               color='orange', edgecolor='black')\\n",
        "        ax6.set_xticks(range(len(emissions_by_mode)))\\n",
        "        ax6.set_xticklabels([m.replace(' ', '\\n') for m in emissions_by_mode.keys()],\\n",
        "                            fontsize=8, rotation=0)\\n",
        "        ax6.set_title('Emissions by Shipping Mode', fontweight='bold')\\n",
        "        ax6.set_ylabel('Emissions (kg CO2)')\\n",
        "\\n",
        "        # 7. Sales Trend Over Time\\n",
        "        ax7 = fig.add_subplot(gs[2, :2])\\n",
        "        monthly_sales = df.groupby(df['order date (DateOrders)'].dt.to_period('M'))['Sales'].sum()\\n",
        "        ax7.plot(range(len(monthly_sales)), monthly_sales.values,\\n",
        "                marker='o', linewidth=2, color='darkgreen')\\n",
        "        ax7.set_title('Monthly Sales Trend', fontweight='bold')\\n",
        "        ax7.set_xlabel('Month Index')\\n",
        "        ax7.set_ylabel('Total Sales ($)')\\n",
        "        ax7.grid(True, alpha=0.3)\\n",
        "\\n",
        "        # 8. Demand Heatmap (simplified)\\n",
        "        ax8 = fig.add_subplot(gs[2, 2])\\n",
        "        top_5_products = df['product'].value_counts().head(5).index\\n",
        "        heatmap_data = df[df['product'].isin(top_5_products)].groupby(\\n",
        "            ['product', df['order date (DateOrders)'].dt.dayofweek]\\n",
        "        )['Order Item Quantity'].sum().unstack(fill_value=0)\\n",
        "\\n",
        "        if not heatmap_data.empty:\\n",
        "            sns.heatmap(heatmap_data, cmap='YlOrRd', ax=ax8, cbar_kws={'label': 'Quantity'})\\n",
        "            ax8.set_title('Demand by Day of Week', fontweight='bold')\\n",
        "            ax8.set_xlabel('Day of Week')\\n",
        "            ax8.set_ylabel('Product ID')\\n",
        "\\n",
        "        plt.suptitle('DASCO Enhanced Supply Chain Analytics Dashboard',\\n",
        "                    fontsize=18, fontweight='bold', y=0.995)\\n",
        "\\n",
        "        plt.savefig('dasco_dashboard.png', dpi=300, bbox_inches='tight')\\n",
        "        print(\"  \u2713 Dashboard saved: dasco_dashboard.png\")\\n",
        "        plt.show()\\n",
        "\\n",
        "\\n",
        "def main():\\n",
        "    print(\"=== Decentralized Autonomous Supply Chain Organization (DASCO) ===\\n\")\\n",
        "\\n",
        "    # 1) Setup blockchain\\n",
        "    w3, contract, account = setup_mock_blockchain()\\n",
        "    if w3 is None:\\n",
        "        print(\"Failed to setup blockchain. Exiting...\")\\n",
        "        return\\n",
        "    print(f\"Mock Blockchain ready. Deployer account: {account}\\n\")\\n",
        "\\n",
        "    # 2) Load dataset\\n",
        "    df, le = load_data('/content/DataCoSupplyChainDataset.csv', min_qty_filter=None)\\n",
        "    if df is None:\\n",
        "        print(\"Dataset not loaded. Exiting.\")\\n",
        "        return\\n",
        "\\n",
        "    # --- NEW: Anomaly Detection ---\\n",
        "    print(\"\\n--- Feature 1: Anomaly Detection ---\")\\n",
        "    ad = AnomalyDetector()\\n",
        "    ad.train(df)\\n",
        "    df = ad.detect(df)\\n",
        "\\n",
        "    # 3) Train ML models\\n",
        "    lstm_model, lstm_product_id, lstm_scaler = train_lstm(df, product_id=None, lookback=30, epochs=20)\\n",
        "    rf_model, rf_columns, rf_scaler = train_rf(df)\\n",
        "\\n",
        "    if lstm_model is None or lstm_product_id is None:\\n",
        "        print(\"LSTM model not trained due to insufficient data. Continuing with RF & blockchain demo.\\n\")\\n",
        "\\n",
        "    # 4) Make predictions / compute optimal stock\\n",
        "    predicted_demand = None\\n",
        "    if lstm_model is not None:\\n",
        "        X_all, _, _ = prepare_lstm_data(df, lstm_product_id, scaler=lstm_scaler, lookback=30)\\n",
        "        if len(X_all) > 0:\\n",
        "            last_window = X_all[-1].reshape((1, 30, 1))\\n",
        "            demand_scaled = float(lstm_model.predict(last_window, verbose=0)[0][0])\\n",
        "            predicted_demand = float(lstm_scaler.inverse_transform([[demand_scaled]])[0][0])\\n",
        "        else:\\n",
        "            print(\"No LSTM windows available for prediction.\")\\n",
        "\\n",
        "    # For RF: create a sample from the first row of df (features must match rf_columns)\\n",
        "    sample_vals = df[rf_columns].iloc[0].values.reshape(1, -1)\\n",
        "    sample_scaled = rf_scaler.transform(sample_vals)\\n",
        "    optimal_stock = float(rf_model.predict(sample_scaled)[0])\\n",
        "\\n",
        "    # --- NEW: Portfolio Optimization ---\\n",
        "    print(\"\\n--- Feature 2: Portfolio Optimization ---\")\\n",
        "    po = PortfolioOptimizer(df, le)\\n",
        "    portfolio_results = po.optimize_portfolio(top_n=5)\\n",
        "\\n",
        "    # --- NEW: Demand Correlation ---\\n",
        "    print(\"\\n--- Feature 3: Demand Correlation ---\")\\n",
        "    dca = DemandCorrelationAnalyzer(df)\\n",
        "    correlations, _ = dca.analyze_correlations(top_n=5)\\n",
        "\\n",
        "    # 5) Emissions recommendation\\n",
        "    product_for_emissions = lstm_product_id if lstm_product_id is not None else df['product'].value_counts().idxmax()\\n",
        "    recommended_mode, emissions_by_mode = optimize_emissions(df, product_for_emissions)\\n",
        "\\n",
        "    # 6) Blockchain demo\\n",
        "    product_name = le.inverse_transform([int(product_for_emissions)])[0]\\n",
        "    timestamp_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\\n",
        "    current_quantity = 2\\n",
        "    threshold = max(1, int(round(optimal_stock)))\\n",
        "\\n",
        "    orders = []\\n",
        "    try:\\n",
        "        order_id = \"TX001\"\\n",
        "        quantity = current_quantity\\n",
        "        emissions = int(quantity * 0.2 * 100)\\n",
        "\\n",
        "        tx_hash = contract.functions.createOrder(order_id, product_name, quantity, emissions).transact({'from': account})\\n",
        "        w3.eth.wait_for_transaction_receipt(tx_hash)\\n",
        "        print(f\"Blockchain: Created order {order_id}\")\\n",
        "\\n",
        "        emissions_update = int(quantity * 0.1 * 100)\\n",
        "        tx_hash = contract.functions.updateShipment(order_id, \"In Transit\", emissions_update).transact({'from': account})\\n",
        "        w3.eth.wait_for_transaction_receipt(tx_hash)\\n",
        "        print(f\"Blockchain: Updated shipment for {order_id}\")\\n",
        "\\n",
        "        if current_quantity < threshold:\\n",
        "            new_order_id = f\"REORDER_{int(time.time())}\"\\n",
        "            emissions_reorder = int((threshold - current_quantity) * 0.1 * 100)\\n",
        "            tx_hash = contract.functions.automateReorder(order_id, product_name, current_quantity, threshold, new_order_id, emissions_reorder).transact({'from': account})\\n",
        "            w3.eth.wait_for_transaction_receipt(tx_hash)\\n",
        "            print(f\"Blockchain: Triggered reorder {new_order_id}\")\\n",
        "        else:\\n",
        "            new_order_id = None\\n",
        "\\n",
        "        fetched_ids = [order_id]\\n",
        "        if new_order_id:\\n",
        "            fetched_ids.append(new_order_id)\\n",
        "\\n",
        "        for oid in fetched_ids:\\n",
        "            try:\\n",
        "                od = contract.functions.getOrder(oid).call()\\n",
        "                ts = od[4]\\n",
        "                ts_str = datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') if ts != 0 else \"0\"\\n",
        "                orders.append({\\n",
        "                    'orderId': od[0],\\n",
        "                    'product': od[1],\\n",
        "                    'quantity': od[2],\\n",
        "                    'status': od[3],\\n",
        "                    'timestamp': ts_str,\\n",
        "                    'emissions': od[5] / 100.0,\\n",
        "                    'sender': od[6]\\n",
        "                })\\n",
        "            except Exception:\\n",
        "                continue\\n",
        "\\n",
        "        total_emissions = contract.functions.getTotalEmissions().call() / 100.0\\n",
        "\\n",
        "    except Exception as e:\\n",
        "        print(\"Blockchain Error:\", e)\\n",
        "        return\\n",
        "\\n",
        "    # Display Results\\n",
        "    print(\"\\n=== Results ===\\n\")\\n",
        "    print(\"1) ML Demand Prediction:\")\\n",
        "    if predicted_demand is not None:\\n",
        "        print(f\"   Product: {product_name}\")\\n",
        "        print(f\"   Predicted Demand: {predicted_demand:.2f} units\")\\n",
        "    else:\\n",
        "        print(f\"   LSTM prediction unavailable.\")\\n",
        "    \\n",
        "    print(\"\\n2) Inventory Optimization:\")\\n",
        "    print(f\"   Optimal Stock: {optimal_stock:.2f} units\")\\n",
        "    \\n",
        "    print(\"\\n3) Sustainability Analysis:\")\\n",
        "    print(f\"   Recommended Mode: {recommended_mode}\")\\n",
        "    print(f\"   Total Emissions (Blockchain): {total_emissions:.2f} kg CO2\")\\n",
        "\\n",
        "    print(\"\\n4) Blockchain Transactions:\")\\n",
        "    for o in orders:\\n",
        "        print(f\"   ID: {o['orderId']}, Status: {o['status']}, Emissions: {o['emissions']:.2f}\")\\n",
        "\\n",
        "    # --- NEW: Dashboard ---\\n",
        "    print(\"\\n--- Feature 6: Dashboard ---\")\\n",
        "    VisualizationDashboard.create_dashboard(df, portfolio_results, df)\\n",
        "\\n",
        "    print(\"\\n=== Summary ===\")\\n",
        "    print(\"Integrated System: Blockchain + ML + Analytics + Dashboard\")\\n",
        "\\n",
        "if __name__ == \"__main__\":\\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_bV_TIAlwtu",
        "outputId": "69c4acbe-b755-46b4-9225-8ac2713339e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Decentralized Autonomous Supply Chain Organization (DASCO) ===\n",
            "\n",
            "Attempting to install solc 0.8.0 (if not already installed)...\n",
            "Setting up eth-tester backend...\n",
            "Deployed contract at address: 0xF2E246BB76DF876Cef8b38ae84130F4F55De395b\n",
            "Mock Blockchain ready. Deployer account: 0x7E5F4552091A69125d5DfCb7b8C2659029395Bdf\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-499603115.py:127: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(mean_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with shape: (180519, 54) and 118 unique products\n",
            "No product_id provided. Using most frequent product: 71\n",
            "Training LSTM for product 71 \u2014 epochs=20, lookback=30\n",
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.1302 - val_loss: 0.0262\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0317 - val_loss: 0.0285\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0290 - val_loss: 0.0261\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0298 - val_loss: 0.0264\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0272 - val_loss: 0.0269\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0288 - val_loss: 0.0258\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0280 - val_loss: 0.0260\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0290 - val_loss: 0.0259\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0296 - val_loss: 0.0257\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0274 - val_loss: 0.0279\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0288 - val_loss: 0.0276\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0270 - val_loss: 0.0255\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0277 - val_loss: 0.0258\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0270 - val_loss: 0.0271\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0298 - val_loss: 0.0271\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0304 - val_loss: 0.0266\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0270 - val_loss: 0.0265\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "LSTM Validation MAE: 12.83, RMSE: 16.29\n",
            "RF Test MAE: 0.02, RMSE: 0.15, R2: 0.989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blockchain: Created order TX001 (tx: 75eebc001c50e970f1282d43f9a3761f4d40008720cacdeeaf08ab5b6b1b5188)\n",
            "Blockchain: Updated shipment for TX001 (tx: 1120f470058d541af3454587bdaad824026a8b4c4f45102e60bc2351558e0ae9)\n",
            "\n",
            "=== Results ===\n",
            "\n",
            "1) ML Demand Prediction:\n",
            "   Product: Perfect Fitness Perfect Rip Deck\n",
            "   Predicted Demand (next day / next window): 68.80 units\n",
            "   Calculation Date: 2025-11-05 20:02:38\n",
            "\n",
            "2) Inventory Optimization (via Random Forest):\n",
            "   Product (sample row): Perfect Fitness Perfect Rip Deck\n",
            "   RF-predicted optimal stock (approx): 1.00 units\n",
            "   Current Stock: 2 units\n",
            "   Reorder Threshold (int): 1 units\n",
            "\n",
            "3) Sustainability Analysis:\n",
            "   Recommended Shipping Mode (min total emissions): Standard Shipping\n",
            "   Emissions by Mode (kg CO2): {'Standard Shipping': 0.0, 'Second Class': 3583.25, 'First Class': 3351.0, 'Same Day': 1461.25}\n",
            "   Total Emissions Recorded on Blockchain: 0.60 kg CO2\n",
            "\n",
            "4) Blockchain Transactions (Proof of Green Compliance):\n",
            "   ID: TX001, Product: Perfect Fitness Perfect Rip Deck, Quantity: 2, Status: In Transit, Emissions: 0.60 kg CO2, Timestamp: 2025-11-05 20:02:21, Sender: 0x7E5F4552091A69125d5DfCb7b8C2659029395Bdf\n",
            "\n",
            "=== Summary ===\n",
            "Prototype successfully integrated ML forecasting, an RF inventory estimator, emissions optimization, and a mock Ethereum smart contract enforcing emissions thresholds.\n",
            "Notes: adjust min_qty_filter in load_data() if your dataset becomes too small. Tune LSTM lookback/epochs and RF features for better performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetector:\n",
        "    \"\"\"NEW FEATURE 1: Real-time Anomaly Detection using Isolation Forest\"\"\"\n",
        "    def __init__(self, contamination=0.1):\n",
        "        self.model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def train(self, df):\n",
        "        features = df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']].copy()\n",
        "        features_scaled = self.scaler.fit_transform(features)\n",
        "        self.model.fit(features_scaled)\n",
        "        print(\"\u2713 Anomaly Detector trained\")\n",
        "\n",
        "    def detect(self, df):\n",
        "        features = df[['Order Item Quantity', 'Sales', 'Days for shipping (real)']].copy()\n",
        "        features_scaled = self.scaler.transform(features)\n",
        "        predictions = self.model.predict(features_scaled)\n",
        "        anomaly_scores = self.model.score_samples(features_scaled)\n",
        "\n",
        "        df['is_anomaly'] = predictions == -1\n",
        "        df['anomaly_score'] = anomaly_scores\n",
        "\n",
        "        anomaly_count = df['is_anomaly'].sum()\n",
        "        print(f\"\u2713 Detected {anomaly_count} anomalies ({anomaly_count/len(df)*100:.2f}%)\")\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "9ZMPVU8swTEN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1-ZOMT1ztWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PortfolioOptimizer:\n",
        "    \"\"\"NEW FEATURE 2: Multi-Product Portfolio Optimization\"\"\"\n",
        "    def __init__(self, df, le):\n",
        "        self.df = df\n",
        "        self.le = le\n",
        "\n",
        "    def optimize_portfolio(self, top_n=5):\n",
        "        print(f\"\\n\ud83d\udd0d Analyzing top {top_n} products for portfolio optimization...\")\n",
        "\n",
        "        # Calculate metrics per product\n",
        "        product_stats = self.df.groupby('product').agg({\n",
        "            'Order Item Quantity': ['sum', 'mean', 'std'],\n",
        "            'Sales': 'sum',\n",
        "            'Days for shipping (real)': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        product_stats.columns = ['product', 'total_qty', 'avg_qty', 'std_qty', 'total_sales', 'avg_shipping_days']\n",
        "        product_stats['revenue_per_unit'] = product_stats['total_sales'] / product_stats['total_qty']\n",
        "        product_stats['consistency_score'] = 1 / (1 + product_stats['std_qty'])\n",
        "        product_stats['efficiency_score'] = 1 / (1 + product_stats['avg_shipping_days'])\n",
        "\n",
        "        # Composite score\n",
        "        product_stats['portfolio_score'] = (\n",
        "            0.4 * product_stats['revenue_per_unit'] / product_stats['revenue_per_unit'].max() +\n",
        "            0.3 * product_stats['consistency_score'] +\n",
        "            0.3 * product_stats['efficiency_score']\n",
        "        )\n",
        "\n",
        "        top_products = product_stats.nlargest(top_n, 'portfolio_score')\n",
        "\n",
        "        results = []\n",
        "        for _, row in top_products.iterrows():\n",
        "            product_name = self.le.inverse_transform([int(row['product'])])[0]\n",
        "            results.append({\n",
        "                'product_id': int(row['product']),\n",
        "                'product_name': product_name,\n",
        "                'portfolio_score': row['portfolio_score'],\n",
        "                'total_sales': row['total_sales'],\n",
        "                'avg_qty': row['avg_qty']\n",
        "            })\n",
        "\n",
        "        print(f\"\u2713 Portfolio optimized with {len(results)} products\")\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "7KuJjhklwsom"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DemandCorrelationAnalyzer:\n",
        "    \"\"\"Feature 3: Cross-Product Demand Correlation Analysis\"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def analyze_correlations(self, top_n=10):\n",
        "        print(f\"  Analyzing correlations for top {top_n} products...\")\n",
        "\n",
        "        top_products = self.df['product'].value_counts().head(top_n).index\n",
        "\n",
        "        daily_demand = self.df[self.df['product'].isin(top_products)].pivot_table(\n",
        "            index='order date (DateOrders)',\n",
        "            columns='product',\n",
        "            values='Order Item Quantity',\n",
        "            aggfunc='sum'\n",
        "        ).fillna(0)\n",
        "\n",
        "        correlation_matrix = daily_demand.corr()\n",
        "\n",
        "        correlations = []\n",
        "        for i in range(len(correlation_matrix)):\n",
        "            for j in range(i+1, len(correlation_matrix)):\n",
        "                correlations.append({\n",
        "                    'product1': correlation_matrix.index[i],\n",
        "                    'product2': correlation_matrix.columns[j],\n",
        "                    'correlation': correlation_matrix.iloc[i, j]\n",
        "                })\n",
        "\n",
        "        correlations_df = pd.DataFrame(correlations)\n",
        "        correlations_df = correlations_df.sort_values('correlation', ascending=False)\n",
        "\n",
        "        print(f\"  \u2713 Found {len(correlations_df)} product correlations\")\n",
        "        return correlations_df.head(5), correlation_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "Ow2CQKo5zupw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizationDashboard:\n",
        "    \"\"\"Feature 6: Advanced Visualization Dashboard\"\"\"\n",
        "    @staticmethod\n",
        "    def create_dashboard(df, portfolio_results, anomaly_df):\n",
        "        print(\"  Creating dashboard...\")\n",
        "\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "        # 1. Order Quantity Distribution\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        ax1.hist(df['Order Item Quantity'], bins=50, color='skyblue', edgecolor='black')\n",
        "        ax1.set_title('Order Quantity Distribution', fontweight='bold')\n",
        "        ax1.set_xlabel('Quantity')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "\n",
        "        # 2. Top Products by Sales\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        top_sales = df.groupby('Product Name')['Sales'].sum().nlargest(8)\n",
        "        ax2.barh(range(len(top_sales)), top_sales.values, color='coral')\n",
        "        ax2.set_yticks(range(len(top_sales)))\n",
        "        ax2.set_yticklabels([name[:25] + '...' if len(name) > 25 else name for name in top_sales.index], fontsize=8)\n",
        "        ax2.set_title('Top 8 Products by Sales', fontweight='bold')\n",
        "        ax2.set_xlabel('Total Sales ($)')\n",
        "\n",
        "        # 3. Shipping Mode Pie Chart\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        shipping_dist = df['Shipping Mode'].value_counts()\n",
        "        colors = plt.cm.Set3(range(len(shipping_dist)))\n",
        "        ax3.pie(shipping_dist.values, labels=shipping_dist.index, autopct='%1.1f%%',\n",
        "               startangle=90, colors=colors)\n",
        "        ax3.set_title('Shipping Mode Distribution', fontweight='bold')\n",
        "\n",
        "        # 4. Anomaly Detection Scatter\n",
        "        ax4 = fig.add_subplot(gs[1, 0])\n",
        "        if 'is_anomaly' in anomaly_df.columns:\n",
        "            normal = anomaly_df[anomaly_df['is_anomaly'] == False]\n",
        "            anomaly = anomaly_df[anomaly_df['is_anomaly'] == True]\n",
        "            ax4.scatter(normal['Sales'], normal['Order Item Quantity'],\n",
        "                       alpha=0.3, s=5, label='Normal', color='blue')\n",
        "            ax4.scatter(anomaly['Sales'], anomaly['Order Item Quantity'],\n",
        "                       alpha=0.8, s=30, label='Anomaly', color='red', marker='x')\n",
        "            ax4.set_title('Anomaly Detection Results', fontweight='bold')\n",
        "            ax4.set_xlabel('Sales ($)')\n",
        "            ax4.set_ylabel('Order Quantity')\n",
        "            ax4.legend()\n",
        "\n",
        "        # 5. Portfolio Scores\n",
        "        ax5 = fig.add_subplot(gs[1, 1])\n",
        "        if portfolio_results:\n",
        "            portfolio_df = pd.DataFrame(portfolio_results)\n",
        "            colors_portfolio = plt.cm.Greens(np.linspace(0.4, 0.8, len(portfolio_df)))\n",
        "            ax5.barh(range(len(portfolio_df)), portfolio_df['portfolio_score'], color=colors_portfolio)\n",
        "            ax5.set_yticks(range(len(portfolio_df)))\n",
        "            ax5.set_yticklabels([name[:20] + '...' if len(name) > 20 else name\n",
        "                                for name in portfolio_df['product_name']], fontsize=8)\n",
        "            ax5.set_title('Portfolio Optimization Scores', fontweight='bold')\n",
        "            ax5.set_xlabel('Score')\n",
        "\n",
        "        # 6. Emissions by Mode\n",
        "        ax6 = fig.add_subplot(gs[1, 2])\n",
        "        emissions_rates = {'Standard Shipping': 0.2, 'Second Class': 0.25,\n",
        "                          'First Class': 0.3, 'Same Day': 0.35}\n",
        "        emissions_by_mode = {}\n",
        "        for mode, rate in emissions_rates.items():\n",
        "            qty = df[df['Shipping Mode'] == mode]['Order Item Quantity'].sum()\n",
        "            emissions_by_mode[mode] = qty * rate\n",
        "\n",
        "        ax6.bar(range(len(emissions_by_mode)), list(emissions_by_mode.values()),\n",
        "               color='orange', edgecolor='black')\n",
        "        ax6.set_xticks(range(len(emissions_by_mode)))\n",
        "        ax6.set_xticklabels([m.replace(' ', '\\n') for m in emissions_by_mode.keys()],\n",
        "                            fontsize=8, rotation=0)\n",
        "        ax6.set_title('Emissions by Shipping Mode', fontweight='bold')\n",
        "        ax6.set_ylabel('Emissions (kg CO2)')\n",
        "\n",
        "        # 7. Sales Trend Over Time\n",
        "        ax7 = fig.add_subplot(gs[2, :2])\n",
        "        monthly_sales = df.groupby(df['order date (DateOrders)'].dt.to_period('M'))['Sales'].sum()\n",
        "        ax7.plot(range(len(monthly_sales)), monthly_sales.values,\n",
        "                marker='o', linewidth=2, color='darkgreen')\n",
        "        ax7.set_title('Monthly Sales Trend', fontweight='bold')\n",
        "        ax7.set_xlabel('Month Index')\n",
        "        ax7.set_ylabel('Total Sales ($)')\n",
        "        ax7.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Demand Heatmap (simplified)\n",
        "        ax8 = fig.add_subplot(gs[2, 2])\n",
        "        top_5_products = df['product'].value_counts().head(5).index\n",
        "        heatmap_data = df[df['product'].isin(top_5_products)].groupby(\n",
        "            ['product', df['order date (DateOrders)'].dt.dayofweek]\n",
        "        )['Order Item Quantity'].sum().unstack(fill_value=0)\n",
        "\n",
        "        if not heatmap_data.empty:\n",
        "            sns.heatmap(heatmap_data, cmap='YlOrRd', ax=ax8, cbar_kws={'label': 'Quantity'})\n",
        "            ax8.set_title('Demand by Day of Week', fontweight='bold')\n",
        "            ax8.set_xlabel('Day of Week')\n",
        "            ax8.set_ylabel('Product ID')\n",
        "\n",
        "        plt.suptitle('DASCO Enhanced Supply Chain Analytics Dashboard',\n",
        "                    fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "        plt.savefig('dasco_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"  \u2713 Dashboard saved: dasco_dashboard.png\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "CefNnd6lz6G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}